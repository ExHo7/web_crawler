
# ğŸš€ Async Web Scraper with Aiohttp & BeautifulSoup ğŸ•¸ï¸

Welcome to the **Async Web Scraper** project! This Python-based web scraper is designed to efficiently scrape web content using asynchronous requests powered by `aiohttp`, and beautiful HTML parsing via `BeautifulSoup`. 

It's fast âš¡, easy to use ğŸ› ï¸, and highly customizable ğŸ”§!

---

## ğŸ› ï¸ Features

- **Asynchronous Scraping**: Uses `aiohttp` for non-blocking requests to scrape multiple websites concurrently. Scrape like a pro and save time! â±ï¸
- **BeautifulSoup Integration**: Efficient HTML parsing to extract data from the scraped web pages with ease! ğŸœ
- **User-Agent Randomization**: Avoid blocks by websites with the help of `fake_useragent`. Be anonymous! ğŸ•¶ï¸
- **Parallel Scraping**: Thanks to `asyncio`, scrape hundreds of URLs in parallel without breaking a sweat! ğŸ§ 
- **Progress Bars**: Keep track of your scraping progress with a fun, visual progress bar (courtesy of `tqdm`) ğŸ¯.
- **Customizable Output**: Save your results in either CSV or JSON format with a single option! ğŸ“œ
- **Retry Mechanism**: Integrated retry mechanism using `tenacity` to handle transient issues during scraping ğŸ›¡ï¸.

---

## ğŸ¯ How It's Built

This project uses several key Python libraries to perform efficient and flexible web scraping:

1. **aiohttp**: For handling asynchronous HTTP requests ğŸŒ.
2. **BeautifulSoup**: For parsing and extracting useful data from the HTML of web pages ğŸœ.
3. **tenacity**: To retry failed scraping attempts with exponential backoff â³.
4. **pandas**: For working with CSV files when saving scraped data in tabular format ğŸ“Š.
5. **tqdm**: To show progress bars during scraping, keeping you informed and motivated ğŸš€.
6. **fake_useragent**: For rotating user agents, so websites wonâ€™t detect you as a bot ğŸ¤–.

---

## ğŸ›ï¸ Command-Line Usage

This scraper comes with several handy options you can use. Hereâ€™s how you can run it:

```bash
python web_crawler.py --url <URL> --output <filename> --format <csv/json>
```

### ğŸ’¡ Available Options

- `--url <URL>`: Specify the single URL you want to scrape ğŸŒ.
- `--file <path>`: Provide a file containing multiple URLs to scrape (CSV or JSON) ğŸ“‚.
- `--output <filename>`: Define the output file name (without extension) where your scraped data will be saved ğŸ“.
- `--format <csv/json>`: Choose whether you want your output in CSV or JSON format ğŸ—‚ï¸.
- `--workers <int>`: Specify the number of asynchronous workers (default is 5) ğŸ§‘â€ğŸ”§.
- `--ignore-robots`: Ignore the `robots.txt` restrictions and scrape all pages ğŸ¦¾.
- `--log-level <DEBUG/INFO/WARNING/ERROR>`: Set the logging verbosity level (default is INFO) ğŸ“œ.

### ğŸ•¹ï¸ Example Usage

To scrape a single website and save the output as a JSON file:

```bash
python web_crawler.py --url https://example.com --output results --format json
```

To scrape multiple websites listed in a CSV file with 10 workers:

```bash
python web_crawler.py --file urls.csv --output results --format csv --workers 10
```

To ignore `robots.txt` and be a true web scraping rebel:

```bash
python web_crawler.py --url https://example.com --output results --ignore-robots
```

---

## ğŸ”§ Installation

### 1. Setting Up a Python Virtual Environment (Highly Recommended)

It's a good practice to run the scraper inside a virtual environment to avoid conflicts with other Python projects or system packages.

### 2. Clone the Repository

1. Clone the repository to your local machine:

   ```bash
   git clone https://github.com/your-repo/web-crawler.git
   ```

2. Navigate to the project folder:

   ```bash
   cd web-crawler
   ```

3. Install the required libraries using the `requirements.txt` file:

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ› ï¸ Requirements

Here's a list of the main dependencies used in this project:

```bash
aiohttp==3.10.9
beautifulsoup4==4.12.3
colorama==0.4.6
fake_useragent==1.5.1
pandas==2.2.3
tenacity==9.0.0
tqdm==4.66.5
```

---

## ğŸŒŸ Fun Features

- **Progressive Scraping**: Watch a real-time progress bar while scraping! ğŸ‰
- **Rotating User-Agents**: Change your user-agent with every request for maximum stealth mode! ğŸ•µï¸â€â™‚ï¸.
- **Custom Logging**: Control the verbosity of logging to see whatâ€™s happening under the hood ğŸ–¥ï¸.

---

## ğŸ¤” Got Questions?

If you run into any issues or have questions, feel free to reach out via [GitHub Issues](https://github.com/your-repo/web-crawler/issues) or open a pull request! Let's make scraping fun and easy, together! ğŸ’¡

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

### ğŸš€ Now go forth and scrape the web responsibly! ğŸŒğŸ•¸ï¸
